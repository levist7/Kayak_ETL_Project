{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Plan your trip with Kayak**\n",
        "\n",
        "\n",
        "> *Company's description*\n",
        "\n",
        "Kayak is a travel search engine that helps user plan their next trip at the best price.\n",
        "\n",
        "The company was founded in 2004 by S. Hafner and P. M. English. After a few rounds of fundraising, Kayak was acquired by Booking Holdings which now holds:\n",
        "  \n",
        "\n",
        "*   Booking.com\n",
        "*   Kayak\n",
        "*   Priceline\n",
        "*   Agoda\n",
        "*   RentalCars\n",
        "*   OpenTable\n",
        "\n",
        "With over $300 million revenue a year, Kaya operates in almost all countries and all languages to helo their users book travels across the globe. \n",
        "\n",
        "\n",
        "\n",
        "> *Project*\n",
        "\n",
        "The marketing team needs help on a new project where users can have more information about the planned destination that they are visiting.\n",
        "\n",
        "Kayak Marketing Team would like to create an application that will recommend where people should plan their next holidays. The application should be based on real data:\n",
        "\n",
        "*   Weather\n",
        "*   Hotels in the area\n",
        "\n",
        "The application should then be able to recommend the best destinations and hotels based on the those variables at any given time.\n",
        "\n",
        "**Goals**\n",
        "\n",
        "Your job will be to: \n",
        "*   Scrape data from destinations\n",
        "*   Get weather data from each destination\n",
        "*   Get hotels' information about each destination\n",
        "*   Store all the information above in a data lake\n",
        "*   Extract, transform and load cleaned data from your datalake to a data warehouse\n",
        "\n",
        "**Scope of this project**\n",
        "\n",
        "Marketing team wants to focus first on the best cities to travel to in France. Here are the top 35 cities according to One-Week-In.com:\n",
        "\n",
        "    Mont Saint Michel\n",
        "    St Malo\n",
        "    Bayeux\n",
        "    Le Havre\n",
        "    Rouen\n",
        "    Paris\n",
        "    Amiens\n",
        "    Lille\n",
        "    Strasbourg\n",
        "    Chateau du Haut Koenigsbourg\n",
        "    Colmar\n",
        "    Eguisheim\n",
        "    Besancon\n",
        "    Dijon\n",
        "    Annecy\n",
        "    Grenoble\n",
        "    Lyon\n",
        "    Verdon Gorge\n",
        "    Bormes les Mimosas\n",
        "    Cassis\n",
        "    Marseille\n",
        "    Aix en Provence\n",
        "    Avignon\n",
        "    Uzès\n",
        "    Nímes\n",
        "    Aigues Mortes\n",
        "    Saintes Maries de la mer\n",
        "    Collioure\n",
        "    Carcassonne\n",
        "    Ariege\n",
        "    Toulouse\n",
        "    Montauban\n",
        "    Biarritz\n",
        "    Bayonne\n",
        "    La Rochelle\n",
        "\n",
        "**Helpers**\n",
        "\n",
        "\n",
        "Get weather data with an API:\n",
        "*   https://nominatim.org/ or\n",
        "*   https://openweathermap.org/appid\n",
        "\n",
        "Save all the results in a .csv file with name of the cities, unique id.\n",
        "\n",
        "Plot destinations on a map using plotly\n",
        "\n",
        "Scrape Booking.com to obtain the informations:\n",
        "\n",
        "*   hotel name\n",
        "*   Url to booking.com page\n",
        "*   Coordinates of latitude and longitude\n",
        "*   Score given by the website users\n",
        "*   Text description of the hotel\n",
        "\n",
        "Create data lake using S3\n",
        "\n",
        "ETL - create a SQL database using AWS RDS to extract data from S3 and store in new DB\n",
        "\n",
        "**Deliverables**\n",
        "\n",
        "1- A .csv file in an S3 bucket containing enriched information about weather and hotels for each French city\n",
        "\n",
        "2- A SQL Database to get the cleaned data from S3\n",
        "\n",
        "3- Two maps with TOP 5 destinations and TOP 20 hotels in the area"
      ],
      "metadata": {
        "id": "_94PoaaNx-rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly\n",
        "import requests\n",
        "#import boto3\n"
      ],
      "metadata": {
        "id": "nXnNPWZDyoPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_cities = [\n",
        "  \"Mont Saint Michel\", \"St Malo\", \"Bayeux\", \"Le Havre\", \"Rouen\",\n",
        "  \"Paris\", \"Amiens\", \"Lille\", \"Strasbourg\", \"Chateau du Haut Koenigsbourg\",\n",
        "  \"Colmar\", \"Eguisheim\", \"Besancon\", \"Dijon\", \"Annecy\", \"Grenoble\", \"Lyon\",\n",
        "  \"Gorges du Verdon\", \"Bormes les Mimosas\", \"Cassis\", \"Marseille\", \"Aix en Provence\",\n",
        "  \"Avignon\", \"Uzes\", \"Nimes\", \"Aigues Mortes\", \"Saintes Maries de la mer\",\n",
        "  \"Collioure\", \"Carcassonne\", \"Ariege\", \"Toulouse\", \"Montauban\", \"Biarritz\",\n",
        "  \"Bayonne\", \"La Rochelle\"\n",
        "  ]\n"
      ],
      "metadata": {
        "id": "Bm-YBKFF81L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####GERI DON BURAYA\n",
        "wk = \"d_0\""
      ],
      "metadata": {
        "id": "LDVg5KszRfNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for aa in list_cities:\n",
        "\n",
        "  url='https://nominatim.openstreetmap.org/search?format=json&q=' + aa\n",
        "  r1 = requests.get(url)\n",
        "  print(r1.json()[0]['display_name'])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6RrTV0r-lEe",
        "outputId": "918dd989-cb2d-4859-9f69-95e03c1a0fd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mont Saint-Michel, Avancée des Bombardes, Le Mont-Saint-Michel, Avranches, Manche, Normandie, France métropolitaine, 50170, France\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#r1.json()['']"
      ],
      "metadata": {
        "id": "zlDKqIh8BYD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dname = r1.json()[0]['display_name']\n",
        "lon = r1.json()[0]['lon'] \n",
        "lat = r1.json()[0]['lat'] "
      ],
      "metadata": {
        "id": "qK-Ng1ds_FpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2 = requests.get('https://api.openweathermap.org/data/2.5/onecall?lat=' + lat +'&lon='+ lon+'&exclude=current,minutely,hourly' + '&units=metric' +'&appid='+wk).json()\n",
        "\n",
        "\n",
        "#daily.pop : probability of precipitation. The values of the parameter vary between 0 and 1, where 0 is equal to 0%, 1 is equal to 100%\n",
        "#daily.rain (where available) Precipitation volume, mm\n",
        "#daily.wind_speed Wind speed. Units – default: metre/sec, metric: metre/sec, imperial: miles/hour\n",
        "#daily.temp Units – default: kelvin, metric: Celsius, imperial: Fahrenheit. How to change units used\n",
        "#&units={units} \n",
        "    #For temperature in Fahrenheit and wind speed in miles/hour, use units=imperial\n",
        "    #For temperature in Celsius and wind speed in meter/sec, use units=metric\n",
        "    #Temperature in Kelvin and wind speed in meter/sec is used by default, so there is no need to use the units parameter in the API call if you want this\n"
      ],
      "metadata": {
        "id": "OMw6063qCC4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#r2"
      ],
      "metadata": {
        "id": "_jSn6vKVN8Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "r2['daily'][0]\n",
        "```\n",
        "**output**\n",
        "{'clouds': 10,\n",
        " 'dew_point': 4.43,\n",
        " 'dt': 1659787200,\n",
        " 'feels_like': {'day': 23.24, 'eve': 22.6, 'morn': 16.05, 'night': 14.79},\n",
        " 'humidity': 28,\n",
        " 'moon_phase': 0.28,\n",
        " 'moonrise': 1659796440,\n",
        " 'moonset': 1659739860,\n",
        " 'pop': 0,\n",
        " 'pressure': 1025,\n",
        " 'sunrise': 1659761196,\n",
        " 'sunset': 1659814639,\n",
        " 'temp': {'day': 24.05,\n",
        "  'eve': 23.32,\n",
        "  'max': 26.26,\n",
        "  'min': 10.82,\n",
        "  'morn': 16.85,\n",
        "  'night': 15.77},\n",
        " 'uvi': 6.53,\n",
        " 'weather': [{'description': 'clear sky',\n",
        "   'icon': '01d',\n",
        "   'id': 800,\n",
        "   'main': 'Clear'}],\n",
        " 'wind_deg': 41,\n",
        " 'wind_gust': 12.26,\n",
        " 'wind_speed': 8.69}"
      ],
      "metadata": {
        "id": "E8h9v54eODT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " for ii in range (7):\n",
        "  print('------ Day ',ii+1, '-------')\n",
        "  print('day_temp= ',r2['daily'][ii]['temp']['day'])\n",
        "  print('weather_description= ', r2['daily'][ii]['weather'])#['description'])\n",
        "  print('cloud_cover= ',r2['daily'][ii]['clouds'])\n",
        "  print('rain_prop= ',r2['daily'][ii]['pop'])\n",
        "  print('wind_speed= ',r2['daily'][ii]['wind_speed'])\n",
        "  print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCtXWkyvCM7J",
        "outputId": "b5a1bbc2-c6a7-4225-f02e-bf141d8f9f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ Day  1 -------\n",
            "day_temp=  25.35\n",
            "weather_description=  [{'id': 801, 'main': 'Clouds', 'description': 'few clouds', 'icon': '02d'}]\n",
            "cloud_cover=  19\n",
            "rain_prop=  0\n",
            "wind_speed=  8.65\n",
            "\n",
            "------ Day  2 -------\n",
            "day_temp=  27.51\n",
            "weather_description=  [{'id': 800, 'main': 'Clear', 'description': 'clear sky', 'icon': '01d'}]\n",
            "cloud_cover=  7\n",
            "rain_prop=  0\n",
            "wind_speed=  7.8\n",
            "\n",
            "------ Day  3 -------\n",
            "day_temp=  29.18\n",
            "weather_description=  [{'id': 800, 'main': 'Clear', 'description': 'clear sky', 'icon': '01d'}]\n",
            "cloud_cover=  0\n",
            "rain_prop=  0\n",
            "wind_speed=  7.46\n",
            "\n",
            "------ Day  4 -------\n",
            "day_temp=  29.75\n",
            "weather_description=  [{'id': 803, 'main': 'Clouds', 'description': 'broken clouds', 'icon': '04d'}]\n",
            "cloud_cover=  74\n",
            "rain_prop=  0\n",
            "wind_speed=  7.77\n",
            "\n",
            "------ Day  5 -------\n",
            "day_temp=  31.07\n",
            "weather_description=  [{'id': 802, 'main': 'Clouds', 'description': 'scattered clouds', 'icon': '03d'}]\n",
            "cloud_cover=  40\n",
            "rain_prop=  0\n",
            "wind_speed=  7.85\n",
            "\n",
            "------ Day  6 -------\n",
            "day_temp=  33.8\n",
            "weather_description=  [{'id': 802, 'main': 'Clouds', 'description': 'scattered clouds', 'icon': '03d'}]\n",
            "cloud_cover=  48\n",
            "rain_prop=  0\n",
            "wind_speed=  9.02\n",
            "\n",
            "------ Day  7 -------\n",
            "day_temp=  34.89\n",
            "weather_description=  [{'id': 803, 'main': 'Clouds', 'description': 'broken clouds', 'icon': '04d'}]\n",
            "cloud_cover=  55\n",
            "rain_prop=  0\n",
            "wind_speed=  8.23\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2 - Scrapping Booking.com**"
      ],
      "metadata": {
        "id": "Yth4aVMGOPn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#installing scrappy library\n",
        "!pip install scrapy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RcpX2TvRPKnO",
        "outputId": "3b5c9fb7-40f1-460f-f84a-57c115663f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11516 sha256=48d6791dcea8aac2679c2706c73f9ebf8af56d5f8134f630ffeb9563ff3dff1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/18/21/3c6a732eaa69a339198e08bb63b7da2c45933a3428b29ec454\n",
            "Successfully built PyDispatcher\n",
            "Installing collected packages: w3lib, cssselect, zope.interface, requests-file, parsel, jmespath, itemadapter, incremental, hyperlink, cryptography, constantly, Automat, Twisted, tldextract, service-identity, queuelib, pyOpenSSL, PyDispatcher, protego, itemloaders, scrapy\n",
            "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 Twisted-22.4.0 constantly-15.1.0 cryptography-37.0.4 cssselect-1.1.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.7.0 itemloaders-1.0.4 jmespath-1.0.1 parsel-1.6.0 protego-0.2.1 pyOpenSSL-22.0.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.6.2 service-identity-21.1.0 tldextract-3.3.1 w3lib-1.22.0 zope.interface-5.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "import os\n",
        "import logging\n",
        "import requests\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess"
      ],
      "metadata": {
        "id": "Qbh2afZ6OWSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r = requests.get('https://www.booking.com/index.fr.html')\n",
        "r"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeSqnIk2nkZ-",
        "outputId": "a214ae5b-5169-45b4-af35-8ce64c92e98b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python booking.py\n",
        "city = 'La Rochelle'#'Paris'"
      ],
      "metadata": {
        "id": "8DmVbuSNPJME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BookingSpider(scrapy.Spider):\n",
        "    name = \"booking\"\n",
        "\n",
        "    start_urls = ['https://www.booking.com/index.fr.html']\n",
        "\n",
        "    def parse(self, response):\n",
        "        return scrapy.FormRequest.from_response(\n",
        "            response,\n",
        "            formdata={'ss': city},\n",
        "            callback=self.after_search\n",
        "        )\n",
        "\n",
        "    def after_search(self, response):\n",
        "        \n",
        "        booking = response.css('.sr_item')\n",
        "\n",
        "        for data in booking:\n",
        "            yield {\n",
        "                'name': data.css('.sr-hotel__name::text').get(),\n",
        "                'url': 'https://www.booking.com' + data.css('.hotel_name_link').attrib[\"href\"],\n",
        "                'coords': data.css('.sr_card_address_line a').attrib[\"data-coords\"],\n",
        "                'score': data.css('.bui-review-score__badge::text').get(),\n",
        "                'description': data.css('.hotel_desc::text').get()\n",
        "                \n",
        "            }\n",
        "        \n",
        "        \n",
        "        try:\n",
        "            next_page = response.css('a.paging-next').attrib[\"href\"]\n",
        "        except KeyError:\n",
        "            logging.info('No next page. Terminating crawling process.')\n",
        "        else:\n",
        "            yield response.follow(next_page, callback=self.after_search)"
      ],
      "metadata": {
        "id": "qoxDRn9cn_l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"hotels_\" + city.replace(\" \", \"-\") + \".json\"\n",
        "\n",
        "\n",
        "process = CrawlerProcess(settings = {\n",
        "    'USER_AGENT': 'Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
        "    'LOG_LEVEL': logging.INFO,\n",
        "    \"FEEDS\": {\n",
        "        'res/' + filename: {\"format\": \"json\"},\n",
        "    }\n",
        "})\n",
        "\n",
        "process.crawl(BookingSpider)\n",
        "process.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kc0tq8JdoS5F",
        "outputId": "e21c9988-fec1-40d7-8daa-e2743e32edd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:scrapy.utils.log:Scrapy 2.6.2 started (bot: scrapybot)\n",
            "2022-08-07 09:31:30 [scrapy.utils.log] INFO: Scrapy 2.6.2 started (bot: scrapybot)\n",
            "INFO:scrapy.utils.log:Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.7.13 (default, Apr 24 2022, 01:04:09) - [GCC 7.5.0], pyOpenSSL 22.0.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 37.0.4, Platform Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-08-07 09:31:30 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.7.13 (default, Apr 24 2022, 01:04:09) - [GCC 7.5.0], pyOpenSSL 22.0.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 37.0.4, Platform Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\n",
            "INFO:scrapy.crawler:Overridden settings:\n",
            "{'LOG_LEVEL': 20,\n",
            " 'USER_AGENT': 'Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
            "2022-08-07 09:31:30 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'LOG_LEVEL': 20,\n",
            " 'USER_AGENT': 'Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
            "DEBUG:scrapy.utils.log:Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "INFO:scrapy.extensions.telnet:Telnet Password: 0fc59785cf0d01b2\n",
            "2022-08-07 09:31:30 [scrapy.extensions.telnet] INFO: Telnet Password: 0fc59785cf0d01b2\n",
            "INFO:scrapy.middleware:Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-08-07 09:31:30 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "INFO:scrapy.middleware:Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-08-07 09:31:30 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "INFO:scrapy.middleware:Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-08-07 09:31:30 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "INFO:scrapy.middleware:Enabled item pipelines:\n",
            "[]\n",
            "2022-08-07 09:31:30 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "INFO:scrapy.core.engine:Spider opened\n",
            "2022-08-07 09:31:30 [scrapy.core.engine] INFO: Spider opened\n",
            "INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-08-07 09:31:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6025\n",
            "2022-08-07 09:31:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ReactorNotRestartable",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-8b5d0bb2a3e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBookingSpider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scrapy/crawler.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjustPoolsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'REACTOR_THREADPOOL_MAXSIZE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddSystemEventTrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shutdown'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_graceful_stop_reactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \"\"\"\n\u001b[1;32m   1295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_installSignalHandlers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m         \u001b[0mReactorBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReactorBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reallyStartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    838\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorAlreadyRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_startedBefore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorNotRestartable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReactorNotRestartable\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIeHB2yDoqG_",
        "outputId": "914c4c7f-f7a5-4c9d-dcb7-62df45eda48f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<scrapy.settings.Settings object at 0x7ff5b0bdba10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_urls = ['https://www.booking.com/searchresults.html?&no_rooms=1&ac_langcode=en&dest_type=city&ss=Paris%2C+Ile+de+France%2C+France&checkin=2022-08-07&checkout= 2022-08-14&ss_raw=Paris&search_selected=false&order=bayesian_review_score']\n"
      ],
      "metadata": {
        "id": "wIZ3uNPjVXgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formdata={'ss': 'Paris%2C+Ile+de+France%2C+France', 'checkin':'2022-08-07',\n",
        "                  'checkout': '2022-08-14', 'ss_raw':'Paris'},\n",
        " "
      ],
      "metadata": {
        "id": "kwdPcPPsXRcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests as r "
      ],
      "metadata": {
        "id": "u1vrS8-oXxQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = response.xpath('//*[@id=\"main-content\"]/div[2]/div/div[3]/div[1]/div[1]/div[3]/div[2]/div[2]/div/div/div/div[7]/div[%s]/div[1]/div[2]/div/div[1]/div[1]/div/div[1]/div/h3/a/div[1]/text()'%NUM)\n",
        "urls = response.xpath('//*[@id=\"main-content\"]/div[2]/div/div[3]/div[1]/div[1]/div[3]/div[2]/div[2]/div/div/div/div[7]/div[3]/div[1]/div[2]/div/div[1]/div[1]/div/div[1]/div/h3/a')\n",
        "    "
      ],
      "metadata": {
        "id": "9NXmLIGjXUqI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}